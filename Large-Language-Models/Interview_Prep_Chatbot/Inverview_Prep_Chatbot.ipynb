{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481a07e2-b8ed-4345-ac95-aa14eed51b95",
   "metadata": {},
   "source": [
    "# Interview Prep Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3e2cb-09d9-460f-baf2-d49ff169ccfe",
   "metadata": {},
   "source": [
    "## Linked with OpenAI API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c17da5c-ba2c-46ed-847d-2ee9dde7104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a84c06-ab55-4fcd-9c9d-6ba76af6cde7",
   "metadata": {},
   "source": [
    "## Loading Below libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eda7ab4-5bde-49cd-8834-1ac38b86456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408caee8-3c58-4eb7-8b06-b5343de530c3",
   "metadata": {},
   "source": [
    "## Setting Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fedf7c-feb7-4314-a6c4-2a3c4a940911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "TOPIC = \"\"\n",
    "USER_QUESTION = \"\"\n",
    "USER_ANSWER = \"\"\n",
    "USER_RATING = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de048ea5-18af-4943-965a-b7f31a61bd85",
   "metadata": {},
   "source": [
    "## Setting the Evaluation Martix Class for Answer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f55c27-4b36-4176-83bd-9584ed8e1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation class matrix definition\n",
    "class Evaluation(BaseModel):\n",
    "    core_understanding: int = Field(description=\"Score out of 5\")\n",
    "    practical_understanding: int = Field(description=\"Score out of 5\")\n",
    "    techincal_precision: int = Field(description=\"Score out of 5\")\n",
    "    common_pitfall: int = Field(description=\"Score out of 5\")\n",
    "\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d958f-ab4b-4f95-83cb-bf87ed9bdbcd",
   "metadata": {},
   "source": [
    "## Prompt, Model and Parser Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd46f07-0061-42f2-8b10-8883c24cb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Prompt Template definition\n",
    "question_prompt_template = PromptTemplate.from_template('''You are an interviewer.\n",
    "Ask one technical interview question on the topic {topic}.''')\n",
    "\n",
    "# Evaluation Prompt Template definition\n",
    "evaluation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Evaluate the answer strictly and return JSON.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\"\"\").partial(format_instructions=pydantic_parser.get_format_instructions())\n",
    "\n",
    "# Chat Model definition\n",
    "chat = ChatOpenAI(\n",
    "    model='gpt-4',\n",
    "    temperature=0,\n",
    "    seed=25\n",
    ")\n",
    "\n",
    "# Output Parser definition\n",
    "str__parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0a38b-f188-4ee7-9a3e-7c7367728bc4",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150d611-b0fa-44b5-bc48-d6b10afd4ef1",
   "metadata": {},
   "source": [
    "### 1. `get_topic()` function to get the topic from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a92dc5d5-3a73-430e-bbfe-2a2424fa8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic():\n",
    "    global TOPIC\n",
    "    while True:\n",
    "        user_input = input(\"Enter your topic\").lower().strip()\n",
    "        if user_input and user_input != 'exit':\n",
    "            TOPIC = user_input\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad414ddc-c39e-4c3b-991d-f06d5b2abe7b",
   "metadata": {},
   "source": [
    "### 2. `generate_question()` Function\n",
    "\n",
    "The `generate_question()` function is used to generate a question based on the user-provided topic.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "- **`llm`**  \n",
    "  Chat model used to generate the question.\n",
    "\n",
    "- **`prompt`**  \n",
    "  Prompt template passed to the chat model to guide question generation.\n",
    "\n",
    "- **`parser`**  \n",
    "  Parser that extracts the important text from the model response without any metadata.\n",
    "\n",
    "- **`topic`**  \n",
    "  User-defined topic on which the question will be based.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2759552f-8226-4e34-af61-5b12cbedb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(llm, prompt, parser, topic):\n",
    "    global USER_QUESTION\n",
    "    chain = prompt | llm | parser\n",
    "    question = chain.invoke(topic)\n",
    "    USER_QUESTION = question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423baa46-9823-426d-ae31-115dff699882",
   "metadata": {},
   "source": [
    "### 2. `get_user_answer()` Function\n",
    "\n",
    "The `get_user_answer()` function allows the user to enter the answer of the question\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "- **`question`**  \n",
    "  Question generated by the chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c4ef0c1-6d46-4c43-90e5-6ad10c2ab9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_answer(question):\n",
    "    global USER_ANSWER\n",
    "    print(f\"Your Question is: {question}\")\n",
    "    user_answer = input(\"Enter your answer\").lower()\n",
    "    USER_ANSWER = user_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7ff6f-50ef-410e-b2d0-48cceccc53d2",
   "metadata": {},
   "source": [
    "### 2. `show_rating()` Function\n",
    "\n",
    "The `show_rating()` function will print the evalution in nice format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6292d12e-dc91-45b8-8c69-3841c94f4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rating():\n",
    "    print(f\"Your Question: {USER_QUESTION}\")\n",
    "    print('-'*10)\n",
    "    print(f\"Your Answer: {USER_ANSWER.capitalize()}\")\n",
    "    print('-'*10)\n",
    "    for i in USER_RATING:\n",
    "        print(f\"{' '.join(i[0].split('_')).capitalize()}: {'⭐'*i[1]} ({i[1]}/5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cab1b8-1383-4e95-a702-929e07ba111c",
   "metadata": {},
   "source": [
    "### 2. `eval_resonse()` Function\n",
    "\n",
    "The `eval_resonse()` function is used to evalute the user answer and model generated question using the chat model.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "- **`llm`**  \n",
    "  Chat model used to generate the question.\n",
    "\n",
    "- **`prompt`**  \n",
    "  Prompt template passed to the chat model to guide question generation.\n",
    "\n",
    "- **`parser`**  \n",
    "  Parser that extracts the important text from the model response without any metadata.\n",
    "\n",
    "- **`question`**  \n",
    "  Question generated by the model\n",
    "\n",
    "- **`answer`**  \n",
    "  Answer entered by the user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57089c1f-bda2-4018-bd4a-1773b0e1a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_resonse(llm, prompt, parser, question, answer):\n",
    "    global USER_RATING\n",
    "    chain = prompt | llm | parser\n",
    "    result = chain.invoke({\n",
    "    'question': question,\n",
    "    'answer' : answer\n",
    "    })\n",
    "\n",
    "    USER_RATING = result\n",
    "        \n",
    "    print(\"Your Answer has been successfully Evaluated ✅\")\n",
    "    show_rating()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa850b4-dc44-47ae-be4e-3c5a214cd389",
   "metadata": {},
   "source": [
    "### 2. `start_chatbot()` Function\n",
    "\n",
    "The `start_chatbot()` function is used to start the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd78d7d-e3ff-4cc2-a891-b8202ede199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_chatbot():\n",
    "    get_topic()\n",
    "    generate_question(llm=chat, prompt=question_prompt_template, parser=str__parser, topic=TOPIC)\n",
    "    clear_output()\n",
    "    answer = get_user_answer(question=USER_QUESTION)\n",
    "    clear_output()\n",
    "    eval_resonse(llm=chat, prompt=evaluation_prompt, parser=pydantic_parser, question=USER_QUESTION, answer=USER_ANSWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9581f090-37e5-44b0-8759-f3db6dfca588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Answer has been successfully Evaluated ✅\n",
      "Your Question: Can you explain how Langchain utilizes OpenAI's GPT-3 model to generate human-like text and what potential applications this technology might have?\n",
      "----------\n",
      "Your Answer: Langchain integrates openai’s gpt-3 by structuring prompts, chaining model calls, and managing context to generate coherent, human-like text. it enables applications such as chatbots, document question answering, automated content generation, interview simulators, agents, and workflow automation powered by large language models.\n",
      "----------\n",
      "Core understanding: ⭐⭐⭐⭐⭐ (5/5)\n",
      "Practical understanding: ⭐⭐⭐⭐⭐ (5/5)\n",
      "Techincal precision: ⭐⭐⭐⭐⭐ (5/5)\n",
      "Common pitfall: ⭐⭐⭐⭐⭐ (5/5)\n"
     ]
    }
   ],
   "source": [
    "start_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d6143-6e82-472f-822a-386f0c611a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain_env)",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
